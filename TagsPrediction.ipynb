{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3573f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "from scipy import sparse as sp_sparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# Below are the functions that will contribute to the pre-processing of the dataset.\n",
    "def special_characters_removal(text, remove_digits=False):\n",
    "\tpattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "\ttext = re.sub(pattern, '', text)\n",
    "\treturn text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    return ' '.join(no_stopword_text)\n",
    "\n",
    "def create_bow(text, word_ind, dict_size):\n",
    "    res_vec = np.zeros(dict_size)\n",
    "    for rec in text.split(' '):\n",
    "        if rec in word_ind:\n",
    "            res_vec[word_ind[rec]] +=1\n",
    "    return res_vec\n",
    "\n",
    "#Dataset is in json format therefore I shall be using the below method to load the data into the data structure before \n",
    "#going further\n",
    "with open('sampleJobDataWithTags.json', encoding=\"utf-8\") as dataset_json:\n",
    "\ttraining_dataset = json.load(dataset_json)\n",
    "\n",
    "preprocessed_dataset = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Data cleansing\n",
    "for datarecords in training_dataset:\n",
    "    \n",
    "    # The BeautifulSoap library helps to scrape data from webpages and provides with the html parser to get the text from \n",
    "    # html which is exactly what is being done below. \n",
    "\tdatarecords[\"title\"] = BeautifulSoup(datarecords[\"title\"], \"html.parser\").get_text()\n",
    "\tdatarecords[\"title\"] = unicodedata.normalize('NFKD', datarecords[\"title\"]).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\tdatarecords[\"title\"]  = special_characters_removal(datarecords[\"title\"])\n",
    "\tdatarecords[\"title\"] = datarecords[\"title\"].lower()\n",
    "    \n",
    "\tdatarecords[\"description\"] = BeautifulSoup(datarecords[\"description\"], \"html.parser\").get_text() \n",
    "\tdatarecords[\"description\"] = unicodedata.normalize('NFKD', datarecords[\"description\"]).encode('ascii', 'ignore').decode('utf-8', 'ignore')    \n",
    "\tdatarecords[\"description\"]  = special_characters_removal(datarecords[\"description\"])\n",
    "\tdatarecords[\"description\"] = datarecords[\"description\"].lower()\n",
    "    \n",
    "    # Performing lemmatization\n",
    "    # First tokenizing and splitting the data into words before performing lemmatization\n",
    "\twordlist_title = nltk.word_tokenize(datarecords[\"title\"])\n",
    "\twordlist_description = nltk.word_tokenize(datarecords[\"description\"])\n",
    "\tdatarecords[\"title\"] = ' '.join([lemmatizer.lemmatize(lemw) for lemw in wordlist_title])\n",
    "\tdatarecords[\"description\"] = ' '.join([lemmatizer.lemmatize(lemw) for lemw in wordlist_description])\n",
    "\n",
    "\t# appending the data\n",
    "\tpreprocessed_dataset.append({\"title\": datarecords[\"title\"], \"description\": datarecords[\"description\"], \"tags\": datarecords[\"tags\"]})\n",
    "\n",
    "print(\"Preprocessed Dataset: \\n\")\n",
    "print(preprocessed_dataset[0:1])\n",
    "\n",
    "# Converting json array to dataframe\n",
    "trainset = pd.read_json(json.dumps(preprocessed_dataset))\n",
    "\n",
    "#Removing of the Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "trainset['description'] = trainset[\"description\"].apply(lambda t: remove_stopwords(t))\n",
    "\n",
    "multilb = MultiLabelBinarizer()\n",
    "multilb.fit(trainset['tags'])\n",
    "\n",
    "# transform target variable\n",
    "target = multilb.transform(trainset['tags'])\n",
    "\n",
    "trainX, testX, ytrain, yval = train_test_split(trainset['description'], target, test_size=0.20, random_state=9)\n",
    "\n",
    "count_of_words = {}\n",
    "for desc in trainX:\n",
    "    for token in desc.split():\n",
    "        if token not in count_of_words:\n",
    "            count_of_words[token] = 1\n",
    "        count_of_words[token] += 1\n",
    "\n",
    "size_of_dict = 10000\n",
    "pop_words = sorted(count_of_words, key=count_of_words.get, reverse=True)[:size_of_dict]\n",
    "words_index = {key: rank for rank, key in enumerate(pop_words, 0)}\n",
    "index_words = {index:word for word, index in words_index.items()}\n",
    "everyword = words_index.keys()\n",
    "\n",
    "trainX_bow = sp_sparse.vstack([sp_sparse.csr_matrix(create_bow(descr, words_index, size_of_dict)) for descr in trainX])\n",
    "testX_bow = sp_sparse.vstack([sp_sparse.csr_matrix(create_bow(descr, words_index, size_of_dict)) for descr in testX])\n",
    "print('X_train shape ', trainX_bow.shape, '\\nX_val shape ', testX_bow.shape)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "lr = LogisticRegression() \n",
    "clf = OneVsRestClassifier(lr)\n",
    "\n",
    "#fitting the model on training data\n",
    "clf.fit(trainX_bow, ytrain)\n",
    "\n",
    "filename='model.pkl'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "filename='multilb.pkl'\n",
    "pickle.dump(multilb, open(filename, 'wb'))\n",
    "\n",
    "filename='wordsindex.pkl'\n",
    "pickle.dump(words_index, open(filename, 'wb'))\n",
    "\n",
    "filename='preprocessed_trainset.pkl'\n",
    "pickle.dump(trainset, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d3dd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Saud Zaman\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Saud Zaman\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Saud Zaman\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training samples 20000\n",
      " * Serving Flask app \"serve_model\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n"
     ]
    }
   ],
   "source": [
    "%run serve_model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2c767e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
