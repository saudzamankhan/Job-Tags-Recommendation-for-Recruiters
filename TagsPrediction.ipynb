{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f84931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import seaborn as sns\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "from scipy import sparse as sp_sparse\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "# Below are the functions that will contribute to the pre-processing of the dataset.\n",
    "def special_characters_removal(text, remove_digits=False):\n",
    "\tpattern = r'[^a-zA-z0-9\\s]' if not remove_digits else r'[^a-zA-z\\s]'\n",
    "\ttext = re.sub(pattern, '', text)\n",
    "\treturn text\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    no_stopword_text = [w for w in text.split() if not w in stop_words]\n",
    "    return ' '.join(no_stopword_text)\n",
    "\n",
    "def create_bow(text, word_ind, dict_size):\n",
    "    res_vec = np.zeros(dict_size)\n",
    "    for rec in text.split(' '):\n",
    "        if rec in word_ind:\n",
    "            res_vec[word_ind[rec]] +=1\n",
    "    return res_vec\n",
    "\n",
    "#Dataset is in json format therefore I shall be using the below method to load the data into the data structure before \n",
    "#going further\n",
    "with open('sampleJobDataWithTags.json', encoding=\"utf-8\") as dataset_json:\n",
    "\ttraining_dataset = json.load(dataset_json)\n",
    "\n",
    "preprocessed_dataset = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Data cleansing\n",
    "for datarecords in training_dataset:\n",
    "    \n",
    "    # The BeautifulSoap library helps to scrape data from webpages and provides with the html parser to get the text from \n",
    "    # html which is exactly what is being done below. \n",
    "\tdatarecords[\"title\"] = BeautifulSoup(datarecords[\"title\"], \"html.parser\").get_text()\n",
    "\tdatarecords[\"title\"] = unicodedata.normalize('NFKD', datarecords[\"title\"]).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\tdatarecords[\"title\"]  = special_characters_removal(datarecords[\"title\"])\n",
    "\tdatarecords[\"title\"] = datarecords[\"title\"].lower()\n",
    "    \n",
    "\tdatarecords[\"description\"] = BeautifulSoup(datarecords[\"description\"], \"html.parser\").get_text() \n",
    "\tdatarecords[\"description\"] = unicodedata.normalize('NFKD', datarecords[\"description\"]).encode('ascii', 'ignore').decode('utf-8', 'ignore')    \n",
    "\tdatarecords[\"description\"]  = special_characters_removal(datarecords[\"description\"])\n",
    "\tdatarecords[\"description\"] = datarecords[\"description\"].lower()\n",
    "    \n",
    "    # Performing lemmatization\n",
    "    # First tokenizing and splitting the data into words before performing lemmatization\n",
    "\twordlist_title = nltk.word_tokenize(datarecords[\"title\"])\n",
    "\twordlist_description = nltk.word_tokenize(datarecords[\"description\"])\n",
    "\tdatarecords[\"title\"] = ' '.join([lemmatizer.lemmatize(lemw) for lemw in wordlist_title])\n",
    "\tdatarecords[\"description\"] = ' '.join([lemmatizer.lemmatize(lemw) for lemw in wordlist_description])\n",
    "\n",
    "\t# appending the data\n",
    "\tpreprocessed_dataset.append({\"title\": datarecords[\"title\"], \"description\": datarecords[\"description\"], \"tags\": datarecords[\"tags\"]})\n",
    "\n",
    "print(\"Preprocessed Dataset: \\n\")\n",
    "print(preprocessed_dataset[0:1])\n",
    "\n",
    "# Converting json array to dataframe\n",
    "trainset = pd.read_json(json.dumps(preprocessed_dataset))\n",
    "\n",
    "#Removing of the Stop Words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "trainset['description'] = trainset[\"description\"].apply(lambda t: remove_stopwords(t))\n",
    "\n",
    "multilb = MultiLabelBinarizer()\n",
    "multilb.fit(trainset['tags'])\n",
    "\n",
    "# transform target variable\n",
    "target = multilb.transform(trainset['tags'])\n",
    "\n",
    "trainX, testX, ytrain, yval = train_test_split(trainset['description'], target, test_size=0.20, random_state=9)\n",
    "\n",
    "count_of_words = {}\n",
    "for desc in trainX:\n",
    "    for token in desc.split():\n",
    "        if token not in count_of_words:\n",
    "            count_of_words[token] = 1\n",
    "        count_of_words[token] += 1\n",
    "\n",
    "size_of_dict = 10000\n",
    "pop_words = sorted(count_of_words, key=count_of_words.get, reverse=True)[:size_of_dict]\n",
    "words_index = {key: rank for rank, key in enumerate(pop_words, 0)}\n",
    "index_words = {index:word for word, index in words_index.items()}\n",
    "everyword = words_index.keys()\n",
    "\n",
    "trainX_bow = sp_sparse.vstack([sp_sparse.csr_matrix(create_bow(descr, words_index, size_of_dict)) for descr in trainX])\n",
    "testX_bow = sp_sparse.vstack([sp_sparse.csr_matrix(create_bow(descr, words_index, size_of_dict)) for descr in testX])\n",
    "print('X_train shape ', trainX_bow.shape, '\\nX_val shape ', testX_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851470bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "lr = LogisticRegression() \n",
    "clf = OneVsRestClassifier(lr)\n",
    "\n",
    "#fitting the model on training data\n",
    "clf.fit(trainX_bow, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335ceb7c",
   "metadata": {},
   "source": [
    "### The trained model along with other pertinent objects needed for the pre-processing of the user inputs on the deployed app are serialized in the script below and saved locally. These saved objects will be loaded before the deployment of the model. The threshold for Logistic Regression is applied after the model is loaded in the 'serve_model.py' script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac4643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename='model.pkl'\n",
    "pickle.dump(clf, open(filename, 'wb'))\n",
    "\n",
    "filename='multilb.pkl'\n",
    "pickle.dump(multilb, open(filename, 'wb'))\n",
    "\n",
    "filename='wordsindex.pkl'\n",
    "pickle.dump(words_index, open(filename, 'wb'))\n",
    "\n",
    "filename='preprocessed_trainset.pkl'\n",
    "pickle.dump(trainset, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aecd310",
   "metadata": {},
   "source": [
    "## Model Serving, Monitoring and Testing (Flask)\n",
    "\n",
    "### The model in the implemented is served using Flask which is the choice of web framework to build a RESTful API. The 'serve_model' Python script is responsible for receiving application user inputs and generating labels predictions using the locally saved objects. The script after generating the predictions, logs the user inputs and the predicted Job description tags locally in a text file and checks for drift from the respective input and target distributions of the training data. Welsh's t-test has been used for this purpose as it allows for comparison among different sample sizes. The test results in a p-value on which a threshold has been defined to check if the distriubtions are different or the same. \n",
    "\n",
    "### Finally, a testing script \"test.py\" is created to send post requests to the endpoints to check for pre-defined user inputs. The script upon execution will generate a prediction and perform the monitoring of the inputs and the predictions. In the script, a Data Scientist Job description taken from LinkedIn has been defined and another one pertains to Analytics and Client Management, a rather diverse role incorporating knowledge from multiple areas and hence variable tags were expected to be predicted. The model predicts the relevant tags for both the inputs very accurately. Even though, the subsequent model monitoring process described the data distributions as different however that difference doesn't lead to the underperforming of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc0adf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Saud Zaman\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Saud Zaman\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Saud Zaman\n",
      "[nltk_data]     Khan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Training samples 20000\n",
      " * Serving Flask app \"serve_model\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n",
      " * Restarting with windowsapi reloader\n"
     ]
    }
   ],
   "source": [
    "#Running this will host the Flask app locally on the port 5000\n",
    "%run serve_model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612eb8d8",
   "metadata": {},
   "source": [
    "## CI CD pipeline (Jenkins, Docker, Github)\n",
    "\n",
    "### Best industry practices have been followed in the building of CI CD pipeline for the application. This involved setting up a Git repository for the application first and then hooking it to Jenkins where a pipeline is set up for the running of the entire process from training the model to deploying it. The 'Jenkinsfile' provided with the submission, scripts the stages that are executed in turns, one by one. First stage of the pipeline builds a docker image, followed by the model training and deployment process in the second stage after which the Flask app with the model is deployed and is accessible on the specified port. Each time, the code changes or data changes are made and pushed to the Git repository, a post request is triggered and is sent to the Web-hook URL which is mapped to the Jenkins endpoint. Jenkins then takes that as a signal to execute the the entire pipeline.  \n",
    "\n",
    "### In the process, Docker is used to perform the containerization of the Flask application \"model_serve.py\" in the system. This involves creating a 'Dockerfile' that sets out a script which is executed when creating an image using the docker build command. The docker run command launches a container that runs a command specified at the end of the 'Dockerfile' script. This command is the one which triggers the model training and deployment process. The docker run command also specifies the port mapping between the container port where the Flask application is accessible and the Localhost's port. The included Dockerfile defines Ubuntu OS as the container's baseline image and writes code to install the Flask application dependencies that enables it to run in the container. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
